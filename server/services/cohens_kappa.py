"""Cohen's Kappa implementation for inter-rater reliability.

Cohen's Kappa is appropriate for:
- Exactly 2 raters
- Categorical data (though can be used with ordinal data)
- Complete data (no missing values)

Formula: Îº = (p_o - p_e) / (1 - p_e)
Where:
- p_o = observed agreement
- p_e = expected agreement by chance
"""

from collections import Counter
from typing import Dict, List, Tuple

from server.models import Annotation


def calculate_cohens_kappa(annotations: List[Annotation]) -> float:
  """Calculate Cohen's Kappa for exactly 2 raters.

  Args:
      annotations: List of annotations from exactly 2 raters

  Returns:
      float: Cohen's Kappa value (-1 to 1)
      - 1.0 = Perfect agreement
      - 0.0 = Agreement equal to chance
      - <0.0 = Systematic disagreement

  Raises:
      ValueError: If not exactly 2 raters or insufficient data

  Example:
      >>> annotations = [
      ...     Annotation(trace_id="t1", user_id="u1", rating=4),
      ...     Annotation(trace_id="t1", user_id="u2", rating=4),
      ...     Annotation(trace_id="t2", user_id="u1", rating=2),
      ...     Annotation(trace_id="t2", user_id="u2", rating=3),
      ... ]
      >>> kappa = calculate_cohens_kappa(annotations)
      >>> # Returns kappa value based on agreement between u1 and u2
  """
  if len(annotations) == 0:
    raise ValueError('No annotations provided')

  # Organize annotations by trace and rater
  data_matrix = _organize_annotations_by_trace_and_rater(annotations)

  # Validate exactly 2 raters
  all_raters = set()
  for trace_ratings in data_matrix.values():
    all_raters.update(trace_ratings.keys())

  if len(all_raters) != 2:
    raise ValueError(f"Cohen's Kappa requires exactly 2 raters, got {len(all_raters)}")

  rater1, rater2 = list(all_raters)

  # Extract paired ratings (only traces rated by both raters)
  paired_ratings = []
  for trace_id, ratings in data_matrix.items():
    if rater1 in ratings and rater2 in ratings:
      paired_ratings.append((ratings[rater1], ratings[rater2]))

  if len(paired_ratings) < 2:
    raise ValueError("Need at least 2 paired ratings to calculate Cohen's Kappa")

  # Calculate observed agreement
  observed_agreement = _calculate_observed_agreement(paired_ratings)

  # Calculate expected agreement by chance
  expected_agreement = _calculate_expected_agreement(paired_ratings)

  # Calculate Cohen's Kappa
  if expected_agreement == 1.0:
    return 1.0 if observed_agreement == 1.0 else 0.0

  kappa = (observed_agreement - expected_agreement) / (1 - expected_agreement)
  return max(-1.0, min(1.0, kappa))  # Clamp to [-1, 1]


def _organize_annotations_by_trace_and_rater(
  annotations: List[Annotation],
) -> Dict[str, Dict[str, int]]:
  """Organize annotations into a matrix: trace_id -> rater_id -> rating.

  Args:
      annotations: List of annotations

  Returns:
      Dict[str, Dict[str, int]]: Nested dictionary of trace->rater->rating
  """
  data_matrix = {}

  for annotation in annotations:
    trace_id = annotation.trace_id
    rater_id = annotation.user_id
    rating = annotation.rating

    if trace_id not in data_matrix:
      data_matrix[trace_id] = {}

    data_matrix[trace_id][rater_id] = rating

  return data_matrix


def _calculate_observed_agreement(paired_ratings: List[Tuple[int, int]]) -> float:
  """Calculate observed agreement between two raters.

  Args:
      paired_ratings: List of (rater1_rating, rater2_rating) tuples

  Returns:
      float: Proportion of ratings where both raters agreed
  """
  total_pairs = len(paired_ratings)
  agreed_pairs = sum(1 for r1, r2 in paired_ratings if r1 == r2)

  return agreed_pairs / total_pairs


def _calculate_expected_agreement(paired_ratings: List[Tuple[int, int]]) -> float:
  """Calculate expected agreement by chance based on marginal distributions.

  Args:
      paired_ratings: List of (rater1_rating, rater2_rating) tuples

  Returns:
      float: Expected agreement by chance

  Mathematical approach:
      For each rating category, calculate the probability that both raters
      would assign that rating by chance, then sum across all categories.
  """
  total_pairs = len(paired_ratings)

  # Count how often each rater used each rating
  rater1_counts = Counter(r1 for r1, r2 in paired_ratings)
  rater2_counts = Counter(r2 for r1, r2 in paired_ratings)

  # Calculate expected agreement for each rating category
  expected_agreement = 0.0

  # Consider all possible rating values (1-5 for Likert scale)
  for rating in range(1, 6):
    p_rater1 = rater1_counts.get(rating, 0) / total_pairs
    p_rater2 = rater2_counts.get(rating, 0) / total_pairs

    # Probability both raters assign this rating by chance
    expected_agreement += p_rater1 * p_rater2

  return expected_agreement


def interpret_cohens_kappa(kappa: float) -> str:
  """Provide human-readable interpretation of Cohen's Kappa score.

  Args:
      kappa: Cohen's Kappa value

  Returns:
      str: Human-readable interpretation

  Interpretation scale based on Landis & Koch (1977):
  - < 0.00: Poor agreement
  - 0.00-0.20: Slight agreement
  - 0.21-0.40: Fair agreement
  - 0.41-0.60: Moderate agreement
  - 0.61-0.80: Substantial agreement
  - 0.81-1.00: Almost perfect agreement
  """
  if kappa < 0.00:
    return 'Poor agreement (systematic disagreement)'
  elif kappa <= 0.20:
    return 'Slight agreement'
  elif kappa <= 0.40:
    return 'Fair agreement'
  elif kappa <= 0.60:
    return 'Moderate agreement'
  elif kappa <= 0.80:
    return 'Substantial agreement'
  else:
    return 'Almost perfect agreement'


def is_cohens_kappa_acceptable(kappa: float, threshold: float = 0.3) -> bool:
  """Check if Cohen's Kappa meets acceptable threshold for workshop progression.

  Args:
      kappa: Cohen's Kappa value
      threshold: Minimum acceptable threshold (default: 0.3)

  Returns:
      bool: True if kappa meets threshold, False otherwise

  Note:
      The default threshold of 0.3 is used for consistency with Krippendorff's Alpha,
      though Cohen's Kappa typically uses different thresholds (e.g., 0.4 for fair agreement).
  """
  return kappa >= threshold
